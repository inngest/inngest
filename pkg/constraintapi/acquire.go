package constraintapi

import (
	"context"
	"crypto/rand"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"time"

	"github.com/google/uuid"
	"github.com/inngest/inngest/pkg/logger"
	"github.com/inngest/inngest/pkg/telemetry/metrics"
	"github.com/inngest/inngest/pkg/util"
	"github.com/inngest/inngest/pkg/util/errs"
	"github.com/oklog/ulid/v2"
	"github.com/redis/rueidis"
)

type stateMetadata struct {
	SourceService           LeaseService      `json:"ss,omitempty"`
	SourceLocation          CallerLocation    `json:"sl,omitempty"`
	SourceRunProcessingMode RunProcessingMode `json:"sm,omitempty"`
}

// redisRequestState represents the data structure stored for every request
// This is used by subsequent calls to Extend, Release to properly handle the lease lifecycle
//
// NOTE: This does not represent one individual lease but is used by
// all leases generated in the Acquire call.
type redisRequestState struct {
	OperationIdempotencyKey string    `json:"k,omitempty"`
	EnvID                   uuid.UUID `json:"e,omitempty"`
	FunctionID              uuid.UUID `json:"f,omitempty"`

	// SortedConstraints represents the list of constraints
	// included in the request sorted to execute in the expected
	// order. Configuration limits are now embedded directly in each constraint.
	SortedConstraints []SerializedConstraintItem `json:"s"`

	// ConfigVersion represents the function version used for this request
	ConfigVersion int `json:"cv,omitempty"`

	// RequestedAmount represents the Amount field in the Acquire request
	RequestedAmount int `json:"r,omitempty"`

	// GrantedAmount is populated in Lua during Acquire and represents the actual capacity granted to the request (how many leases were generated)
	GrantedAmount int `json:"g,omitempty"`

	// ActiveAmount represents the total number of active leases (where Release was not yet called)
	ActiveAmount int `json:"a,omitempty"`

	// MaximumLifetime is optional and represenst the maximum lifetime for leases generated by this request.
	// This is enforced during ExtendLease.
	MaximumLifetimeMillis int64 `json:"l,omitempty"`

	// HashedLeaseIdempotencyKeys stores the hashed idempotency keys used to generate leases
	HashedLeaseIdempotencyKeys []string `json:"lik,omitempty"`

	// LeaseRunIDs stores the run IDs associated with hashed lease idempotency keys
	LeaseRunIDs map[string]ulid.ULID `json:"lri,omitempty"`

	// Annotate metadata
	Metadata stateMetadata `json:"m,omitzero"`
}

func buildRequestState(req *CapacityAcquireRequest) (
	[]byte,
	[]ConstraintItem,
	map[string]string,
	string,
	error,
) {
	state := &redisRequestState{
		OperationIdempotencyKey: req.IdempotencyKey,
		EnvID:                   req.EnvID,
		FunctionID:              req.FunctionID,
		RequestedAmount:         req.Amount,
		MaximumLifetimeMillis:   req.MaximumLifetime.Milliseconds(),
		ConfigVersion:           req.Configuration.FunctionVersion,

		// These keys are set during Acquire and Release respectively
		GrantedAmount: 0,
		ActiveAmount:  0,

		Metadata: stateMetadata{
			SourceService:           req.Source.Service,
			SourceLocation:          req.Source.Location,
			SourceRunProcessingMode: req.Source.RunProcessingMode,
		},
	}

	// We hash all idempotency keys provided by users internally.
	// This is a security precaution and helps reduce memory usage.
	// Users still expect the unhashed idempotency keys to be returned,
	// hence we need to track the mapping between idempotency key -> hash(idempotency key)
	//
	// NOTE: If the lease idempotency keys change between requests with the same idempotency key
	// (which should never happen), this should trigger the fingerprint below to change.
	hashedLeaseIdempotencyKeysMap := make(map[string]string)
	hashedLeaseIdempotencyKeys := make([]string, len(req.LeaseIdempotencyKeys))
	for i, leaseIdempotencyKey := range req.LeaseIdempotencyKeys {
		hashed := util.XXHash(leaseIdempotencyKey)
		hashedLeaseIdempotencyKeys[i] = hashed
		hashedLeaseIdempotencyKeysMap[hashed] = leaseIdempotencyKey
	}

	state.HashedLeaseIdempotencyKeys = hashedLeaseIdempotencyKeys

	// Ensure to hash lease idempotency key in run ID map
	leaseRunIDs := make(map[string]ulid.ULID)
	for k, u := range req.LeaseRunIDs {
		leaseRunIDs[util.XXHash(k)] = u
	}

	state.LeaseRunIDs = leaseRunIDs

	// Sort and serialize constraints with embedded configuration limits
	constraints := req.Constraints
	sortConstraints(constraints)

	serialized := make([]SerializedConstraintItem, len(constraints))
	for i := range constraints {
		serialized[i] = constraints[i].ToSerializedConstraintItem(
			req.Configuration,
			req.AccountID,
			req.EnvID,
			req.FunctionID,
		)
	}

	state.SortedConstraints = serialized

	dataBytes, err := json.Marshal(state)
	if err != nil {
		return nil, nil, nil, "", fmt.Errorf("could not marshal request: %w", err)
	}

	// NOTE: We fingerprint the query to waive idempotency in case the configuration,
	// requested constraints, etc. changed.
	var hash string
	{
		fingerprint := sha256.New()
		_, err = fingerprint.Write(dataBytes)
		if err != nil {
			return nil, nil, nil, "", fmt.Errorf("could not fingerprint query: %w", err)
		}
		hash = hex.EncodeToString(fingerprint.Sum(nil))
	}

	return dataBytes, constraints, hashedLeaseIdempotencyKeysMap, hash, nil
}

type acquireScriptResponse struct {
	Status        int `json:"s"`
	Requested     int `json:"r"`
	Granted       int `json:"g"`
	GrantedLeases []struct {
		LeaseID             ulid.ULID `json:"lid"`
		LeaseIdempotencyKey string    `json:"lik"`
	} `json:"l"`
	LimitingConstraints  flexibleIntArray    `json:"lc"`
	ExhaustedConstraints flexibleIntArray    `json:"ec"`
	FairnessReduction    int                 `json:"fr"`
	RetryAt              int                 `json:"ra"`
	Debug                flexibleStringArray `json:"d"`
}

func (r *redisCapacityManager) Acquire(ctx context.Context, req *CapacityAcquireRequest) (*CapacityAcquireResponse, errs.InternalError) {
	l := logger.StdlibLogger(ctx)

	requestID, err := ulid.New(ulid.Timestamp(r.clock.Now()), rand.Reader)
	if err != nil {
		return nil, errs.Wrap(0, false, "could not generate request ID: %w", err)
	}

	// Validate request
	if err := req.Valid(); err != nil {
		return nil, errs.Wrap(0, false, "invalid request: %w", err)
	}

	l = l.With(
		"account_id", req.AccountID,
		"env_id", req.EnvID,
		"fn_id", req.FunctionID,
		"req_id", requestID,
		"source", req.Source,
		"shard", r.shardName,
	)

	now := r.clock.Now()

	// NOTE: This will include request latency (marshaling, network delays),
	// and it might not work for retries, as those retain the same CurrentTime value.
	requestLatency := now.Sub(req.CurrentTime)

	metrics.HistogramConstraintAPIRequestLatency(ctx, requestLatency, metrics.HistogramOpt{
		PkgName: pkgName,
		Tags: map[string]any{
			"operation": "acquire",
			"attempt":   req.RequestAttempt,
			"shard":     r.shardName,
		},
	})

	if requestLatency > MaximumAllowedRequestDelay {
		// TODO : Set proper error code
		return nil, errs.Wrap(
			0,
			false,
			"exceeded maximum allowed request delay, latency: %s, attempt: %d", requestLatency, req.RequestAttempt,
		)
	}

	enableHighCardinalityInstrumentation := r.enableHighCardinalityInstrumentation != nil && r.enableHighCardinalityInstrumentation(ctx, req.AccountID, req.EnvID, req.FunctionID)

	// TODO: Should we get the current time again/cancel if too much time passed up until here?
	leaseExpiry := now.Add(req.Duration)

	// Generate lease IDs
	initialLeaseIDs := make([]ulid.ULID, len(req.LeaseIdempotencyKeys))
	for i := range req.LeaseIdempotencyKeys {
		leaseID, err := ulid.New(ulid.Timestamp(leaseExpiry), rand.Reader)
		if err != nil {
			return nil, errs.Wrap(0, true, "failed to generate lease IDs: %w", err)
		}
		initialLeaseIDs[i] = leaseID
	}

	requestState, sortedConstraints, hashedLeaseIdempotencyKeysMap, fingerprint, err := buildRequestState(req)
	if err != nil {
		return nil, errs.Wrap(0, false, "could not build request state: %w", err)
	}

	// Build Lua request

	// When the same Acquire request is received again after a successful first request, we will
	// return the same generated lease for a short idempotency period. This is to facilitate
	// graceful retries in case the caller fails to use the returned lease in the first attempt (e.g. OOM).
	//
	// NOTE: We must include the request fingerprint to ensure idempotency is reset
	// in case the configuration changed between requests (e.g. user syncs functions between retries of a queue item)
	operationIdempotencyKey := fmt.Sprintf("%s-%s", req.IdempotencyKey, fingerprint)
	// The idempotency period must always be <= the lease duration, as we may return an expired lease otherwise.
	operationIdempotencyPeriod := min(r.operationIdempotencyTTL, req.Duration)

	keys := []string{
		// The request state is persisted for consistent cleanup during Scavenge/Release operations
		r.keyRequestState(req.AccountID, requestID),

		// Operation idempotency is used for retries while the generated leases are still valid
		r.keyOperationIdempotency(req.AccountID, "acq", operationIdempotencyKey),

		// Enable idempotency for the entire Acquire call. This is used by batch operations like BacklogRefill which
		// may need to skip GCRA checks without knowing individual leases/items
		r.keyConstraintCheckIdempotency(req.AccountID, req.IdempotencyKey),

		r.keyScavengerShard(),

		r.keyAccountLeases(req.AccountID),
	}

	enableDebugLogsVal := "0"
	if enableDebugLogs || r.enableDebugLogs {
		enableDebugLogsVal = "1"
	}

	scopedKeyPrefix := fmt.Sprintf("{cs}:%s", accountScope(req.AccountID))

	args, err := strSlice([]any{
		// This will be marshaled
		rueidis.BinaryString(requestState),
		requestID.String(),

		req.AccountID,
		now.UnixMilli(), // current time in milliseconds for throttle
		now.UnixNano(),  // current time in nanoseconds for rate limiting

		leaseExpiry.UnixMilli(),
		scopedKeyPrefix,
		initialLeaseIDs,

		int(operationIdempotencyPeriod.Seconds()),
		int(r.constraintCheckIdempotencyTTL.Seconds()),

		enableDebugLogsVal,
	})
	if err != nil {
		return nil, errs.Wrap(0, false, "invalid args: %w", err)
	}

	l.Trace(
		"prepared acquire call",
	)

	rawRes, internalErr := executeLuaScript(
		ctx,
		"acquire",
		r.shardName,
		req.Source,
		r.client,
		r.clock,
		keys,
		args,
	)
	if internalErr != nil {
		return nil, internalErr
	}

	parsedResponse := acquireScriptResponse{}
	err = json.Unmarshal(rawRes, &parsedResponse)
	if err != nil {
		return nil, errs.Wrap(0, false, "invalid response structure: %w", err)
	}

	leases := make([]CapacityLease, len(parsedResponse.GrantedLeases))
	for i, v := range parsedResponse.GrantedLeases {
		// NOTE: To return the original lease idempotency key back to the user,
		// we have to perform a reverse lookup in the map of hash(idempotency key) -> idempotency key
		// we created when serializing the request state.
		leaseIdempotencyKey, ok := hashedLeaseIdempotencyKeysMap[v.LeaseIdempotencyKey]
		if !ok {
			return nil, errs.Wrap(0, false, "invalid hashed lease idempotency key returned")
		}

		leases[i] = CapacityLease{
			LeaseID:        v.LeaseID,
			IdempotencyKey: leaseIdempotencyKey,
		}
	}

	var limitingConstraints []ConstraintItem
	if len(parsedResponse.LimitingConstraints) > 0 {
		limitingConstraints = make([]ConstraintItem, len(parsedResponse.LimitingConstraints))
		for i, limitingConstraintIndex := range []int(parsedResponse.LimitingConstraints) {
			limitingConstraints[i] = sortedConstraints[limitingConstraintIndex-1]
		}
	}

	var exhaustedConstraints []ConstraintItem
	if len(parsedResponse.ExhaustedConstraints) > 0 {
		exhaustedConstraints = make([]ConstraintItem, len(parsedResponse.ExhaustedConstraints))
		for i, exhaustedConstraintIndex := range []int(parsedResponse.ExhaustedConstraints) {
			exhaustedConstraints[i] = sortedConstraints[exhaustedConstraintIndex-1]
		}
	}

	retryAfter := time.UnixMilli(int64(parsedResponse.RetryAt))
	if retryAfter.Before(now) {
		retryAfter = time.Time{}
	}

	untilRetryAfter := max(retryAfter.Sub(now), 0)
	metrics.HistogramConstraintAPIRetryAfterDuration(ctx, untilRetryAfter, metrics.HistogramOpt{
		PkgName: pkgName,
		Tags: map[string]any{
			"location":            req.Source.Location.String(),
			"service":             req.Source.Service.String(),
			"run_processing_mode": req.Source.RunProcessingMode.String(),
			"shard":               r.shardName,
		},
	})

	if len(r.lifecycles) > 0 {
		for _, hook := range r.lifecycles {
			err := hook.OnCapacityLeaseAcquired(ctx, OnCapacityLeaseAcquiredData{
				AccountID:            req.AccountID,
				EnvID:                req.EnvID,
				FunctionID:           req.FunctionID,
				Configuration:        req.Configuration,
				Constraints:          req.Constraints,
				LimitingConstraints:  limitingConstraints,
				ExhaustedConstraints: exhaustedConstraints,
				FairnessReduction:    parsedResponse.FairnessReduction,
				RetryAfter:           retryAfter,
				RequestedAmount:      req.Amount,
				Duration:             req.Duration,
				Source:               req.Source,
				GrantedLeases:        leases,
			})
			if err != nil {
				return nil, errs.Wrap(0, false, "acquire lifecycle failed: %w", err)
			}
		}
	}

	l = l.With(
		"status", parsedResponse.Status,
	)

	tags := map[string]any{
		"shard": r.shardName,
	}
	if enableHighCardinalityInstrumentation {
		tags["function_id"] = req.FunctionID
	}

	// Export metrics for leases requested and granted
	metrics.IncrConstraintAPILeasesRequestedCounter(ctx, int64(parsedResponse.Requested), metrics.CounterOpt{
		PkgName: "constraintapi",
		Tags:    tags,
	})

	metrics.IncrConstraintAPILeasesGrantedCounter(ctx, int64(parsedResponse.Granted), metrics.CounterOpt{
		PkgName: "constraintapi",
		Tags:    tags,
	})

	// Export counter for limitingConstraints
	for _, constraint := range limitingConstraints {
		tags["limiting_constraint"] = constraint.MetricsIdentifier()
		metrics.IncrConstraintAPILimitingConstraintsCounter(ctx, metrics.CounterOpt{
			PkgName: "constraintapi",
			Tags:    tags,
		})
	}
	delete(tags, "limiting_constraint")

	// Export counter for exhaustedConstraints
	for _, constraint := range exhaustedConstraints {
		tags["constraint"] = constraint.MetricsIdentifier()
		metrics.IncrConstraintAPIExhaustedConstraintsCounter(ctx, metrics.CounterOpt{
			PkgName: "constraintapi",
			Tags:    tags,
		})
	}
	delete(tags, "constraint")

	switch parsedResponse.Status {
	case 1, 3:
		if enableHighCardinalityInstrumentation {
			l.Debug(
				"successful acquire call",
				"leases", leases,
			)
		}

		metrics.HistogramConstraintAPIRequestStateSize(ctx, int64(len(requestState)), metrics.HistogramOpt{
			PkgName: pkgName,
			Tags: map[string]any{
				"location":            req.Source.Location.String(),
				"service":             req.Source.Service.String(),
				"run_processing_mode": req.Source.RunProcessingMode.String(),
				"shard":               r.shardName,
			},
		})

		metrics.IncrConstraintAPIIssuedLeaseCounter(ctx, int64(len(leases)), metrics.CounterOpt{
			PkgName: pkgName,
			Tags: map[string]any{
				"location":            req.Source.Location.String(),
				"service":             req.Source.Service.String(),
				"run_processing_mode": req.Source.RunProcessingMode.String(),
				"shard":               r.shardName,
			},
		})

		// success or idempotency
		return &CapacityAcquireResponse{
			RequestID:            requestID,
			Leases:               leases,
			LimitingConstraints:  limitingConstraints,
			ExhaustedConstraints: exhaustedConstraints,
			FairnessReduction:    parsedResponse.FairnessReduction,
			RetryAfter:           retryAfter,
			internalDebugState:   parsedResponse,
		}, nil

	case 2:
		l.Trace(
			"acquire call lacking capacity",
			"limiting", limitingConstraints,
			"exhausted", exhaustedConstraints,
		)

		// lacking capacity
		return &CapacityAcquireResponse{
			RequestID:            requestID,
			Leases:               leases,
			LimitingConstraints:  limitingConstraints,
			ExhaustedConstraints: exhaustedConstraints,
			RetryAfter:           retryAfter,
			FairnessReduction:    parsedResponse.FairnessReduction,
			internalDebugState:   parsedResponse,
		}, nil

	case 4:
		l.Trace("acquire while previous request state still exists")
		return nil, errs.Wrap(0, false, "request state for this idempotency key already exists")

	default:
		return nil, errs.Wrap(0, false, "unexpected status code %v", parsedResponse.Status)
	}
}
