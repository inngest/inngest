package constraintapi

import (
	"context"
	"crypto/rand"
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"time"

	"github.com/google/uuid"
	"github.com/inngest/inngest/pkg/logger"
	"github.com/inngest/inngest/pkg/util"
	"github.com/inngest/inngest/pkg/util/errs"
	"github.com/oklog/ulid/v2"
	"github.com/redis/rueidis"
)

// redisRequestState represents the data structure stored for every request
// This is used by subsequent calls to Extend, Release to properly handle the lease lifecycle
//
// NOTE: This does not represent one individual lease but is used by
// all leases generated in the Acquire call.
type redisRequestState struct {
	OperationIdempotencyKey string    `json:"k,omitempty"`
	EnvID                   uuid.UUID `json:"e,omitempty"`
	FunctionID              uuid.UUID `json:"f,omitempty"`

	// SortedConstraints represents the list of constraints
	// included in the request sorted to execute in the expected
	// order. Configuration limits are now embedded directly in each constraint.
	SortedConstraints []SerializedConstraintItem `json:"s"`

	// ConfigVersion represents the function version used for this request
	ConfigVersion int `json:"cv,omitempty"`

	// RequestedAmount represents the Amount field in the Acquire request
	RequestedAmount int `json:"r,omitempty"`

	// GrantedAmount is populated in Lua during Acquire and represents the actual capacity granted to the request (how many leases were generated)
	GrantedAmount int `json:"g,omitempty"`

	// ActiveAmount represents the total number of active leases (where Release was not yet called)
	ActiveAmount int `json:"a,omitempty"`

	// MaximumLifetime is optional and represenst the maximum lifetime for leases generated by this request.
	// This is enforced during ExtendLease.
	MaximumLifetimeMillis int64 `json:"l,omitempty"`

	// HashedLeaseIdempotencyKeys stores the hashed idempotency keys used to generate leases
	HashedLeaseIdempotencyKeys []string `json:"lik,omitempty"`

	// LeaseRunIDs stores the run IDs associated with hashed lease idempotency keys
	LeaseRunIDs map[string]ulid.ULID `json:"lri,omitempty"`
}

func buildRequestState(req *CapacityAcquireRequest, keyPrefix string) (
	[]byte,
	[]ConstraintItem,
	map[string]string,
	string,
	error,
) {
	state := &redisRequestState{
		OperationIdempotencyKey: req.IdempotencyKey,
		EnvID:                   req.EnvID,
		FunctionID:              req.FunctionID,
		RequestedAmount:         req.Amount,
		MaximumLifetimeMillis:   req.MaximumLifetime.Milliseconds(),
		ConfigVersion:           req.Configuration.FunctionVersion,

		// These keys are set during Acquire and Release respectively
		GrantedAmount: 0,
		ActiveAmount:  0,
	}

	// We hash all idempotency keys provided by users internally.
	// This is a security precaution and helps reduce memory usage.
	// Users still expect the unhashed idempotency keys to be returned,
	// hence we need to track the mapping between idempotency key -> hash(idempotency key)
	//
	// NOTE: If the lease idempotency keys change between requests with the same idempotency key
	// (which should never happen), this should trigger the fingerprint below to change.
	hashedLeaseIdempotencyKeysMap := make(map[string]string)
	hashedLeaseIdempotencyKeys := make([]string, len(req.LeaseIdempotencyKeys))
	for i, leaseIdempotencyKey := range req.LeaseIdempotencyKeys {
		hashed := util.XXHash(leaseIdempotencyKey)
		hashedLeaseIdempotencyKeys[i] = hashed
		hashedLeaseIdempotencyKeysMap[hashed] = leaseIdempotencyKey
	}

	state.HashedLeaseIdempotencyKeys = hashedLeaseIdempotencyKeys

	// Ensure to hash lease idempotency key in run ID map
	leaseRunIDs := make(map[string]ulid.ULID)
	for k, u := range req.LeaseRunIDs {
		leaseRunIDs[util.XXHash(k)] = u
	}

	state.LeaseRunIDs = leaseRunIDs

	// Sort and serialize constraints with embedded configuration limits
	constraints := req.Constraints
	sortConstraints(constraints)

	serialized := make([]SerializedConstraintItem, len(constraints))
	for i := range constraints {
		serialized[i] = constraints[i].ToSerializedConstraintItem(
			req.Configuration,
			req.AccountID,
			req.EnvID,
			req.FunctionID,
			keyPrefix,
		)
	}

	state.SortedConstraints = serialized

	dataBytes, err := json.Marshal(state)
	if err != nil {
		return nil, nil, nil, "", fmt.Errorf("could not marshal request: %w", err)
	}

	// NOTE: We fingerprint the query to waive idempotency in case the configuration,
	// requested constraints, etc. changed.
	var hash string
	{
		fingerprint := sha256.New()
		_, err = fingerprint.Write(dataBytes)
		if err != nil {
			return nil, nil, nil, "", fmt.Errorf("could not fingerprint query: %w", err)
		}
		hash = hex.EncodeToString(fingerprint.Sum(nil))
	}

	return dataBytes, constraints, hashedLeaseIdempotencyKeysMap, hash, nil
}

type acquireScriptResponse struct {
	Status        int `json:"s"`
	Requested     int `json:"r"`
	Granted       int `json:"g"`
	GrantedLeases []struct {
		LeaseID             ulid.ULID `json:"lid"`
		LeaseIdempotencyKey string    `json:"lik"`
	} `json:"l"`
	LimitingConstraints flexibleIntArray    `json:"lc"`
	FairnessReduction   int                 `json:"fr"`
	RetryAt             int                 `json:"ra"`
	Debug               flexibleStringArray `json:"d"`

	ActiveAccountLeases  int `json:"aal"`
	ExpiredAccountLeases int `json:"eal"`
	EarliestLeaseExpiry  int `json:"ele"`
}

func (r *redisCapacityManager) Acquire(ctx context.Context, req *CapacityAcquireRequest) (*CapacityAcquireResponse, errs.InternalError) {
	l := logger.StdlibLogger(ctx)

	requestID, err := ulid.New(ulid.Timestamp(r.clock.Now()), rand.Reader)
	if err != nil {
		return nil, errs.Wrap(0, false, "could not generate request ID: %w", err)
	}

	// Validate request
	if err := req.Valid(); err != nil {
		return nil, errs.Wrap(0, false, "invalid request: %w", err)
	}

	l = l.With(
		"account_id", req.AccountID,
		"env_id", req.EnvID,
		"fn_id", req.FunctionID,
		"req_id", requestID,
	)

	now := r.clock.Now()

	// TODO: Add metric for this
	// NOTE: This will include request latency (marshaling, network delays),
	// and it might not work for retries, as those retain the same CurrentTime value.
	// TODO: Ensure retries have the updated CurrentTime
	requestLatency := now.Sub(req.CurrentTime)
	if requestLatency > MaximumAllowedRequestDelay {
		// TODO : Set proper error code
		return nil, errs.Wrap(0, false, "exceeded maximum allowed request delay")
	}

	// Retrieve client and key prefix for current constraints
	// NOTE: We will no longer need this once we move to a dedicated store for constraint state
	keyPrefix, client, err := r.clientAndPrefix(req.Migration)
	if err != nil {
		return nil, errs.Wrap(0, false, "failed to get client: %w", err)
	}

	// TODO: Should we get the current time again/cancel if too much time passed up until here?
	leaseExpiry := now.Add(req.Duration)

	// Generate lease IDs
	initialLeaseIDs := make([]ulid.ULID, len(req.LeaseIdempotencyKeys))
	for i := range req.LeaseIdempotencyKeys {
		leaseID, err := ulid.New(ulid.Timestamp(leaseExpiry), rand.Reader)
		if err != nil {
			return nil, errs.Wrap(0, true, "failed to generate lease IDs: %w", err)
		}
		initialLeaseIDs[i] = leaseID
	}

	requestState, sortedConstraints, hashedLeaseIdempotencyKeysMap, fingerprint, err := buildRequestState(req, keyPrefix)
	if err != nil {
		return nil, errs.Wrap(0, false, "could not build request state: %w", err)
	}

	// Deterministically compute this based on numScavengerShards and accountID
	scavengerShard := r.scavengerShard(ctx, req.AccountID)

	// Build Lua request

	// When the same Acquire request is received again after a successful first request, we will
	// return the same generated lease for a short idempotency period. This is to facilitate
	// graceful retries in case the caller fails to use the returned lease in the first attempt (e.g. OOM).
	//
	// NOTE: We must include the request fingerprint to ensure idempotency is reset
	// in case the configuration changed between requests (e.g. user syncs functions between retries of a queue item)
	operationIdempotencyKey := fmt.Sprintf("%s-%s", req.IdempotencyKey, fingerprint)
	// The idempotency period must always be <= the lease duration, as we may return an expired lease otherwise.
	operationIdempotencyPeriod := min(r.operationIdempotencyTTL, req.Duration)

	keys := []string{
		// The request state is persisted for consistent cleanup during Scavenge/Release operations
		r.keyRequestState(keyPrefix, req.AccountID, requestID),

		// Operation idempotency is used for retries while the generated leases are still valid
		r.keyOperationIdempotency(keyPrefix, req.AccountID, "acq", operationIdempotencyKey),

		// Enable idempotency for the entire Acquire call. This is used by batch operations like BacklogRefill which
		// may need to skip GCRA checks without knowing individual leases/items
		r.keyConstraintCheckIdempotency(keyPrefix, req.AccountID, req.IdempotencyKey),

		r.keyScavengerShard(keyPrefix, scavengerShard),
		r.keyAccountLeases(keyPrefix, req.AccountID),
	}

	enableDebugLogsVal := "0"
	if enableDebugLogs || r.enableDebugLogs {
		enableDebugLogsVal = "1"
	}

	scopedKeyPrefix := fmt.Sprintf("{%s}:%s", keyPrefix, accountScope(req.AccountID))

	// TODO: Pass through flag
	enableThrottleCompatibilityModeVal := "0"

	args, err := strSlice([]any{
		// This will be marshaled
		rueidis.BinaryString(requestState),
		requestID.String(),

		req.AccountID,
		now.UnixMilli(), // current time in milliseconds for throttle
		now.UnixNano(),  // current time in nanoseconds for rate limiting

		leaseExpiry.UnixMilli(),
		scopedKeyPrefix,
		initialLeaseIDs,

		int(operationIdempotencyPeriod.Seconds()),
		int(r.constraintCheckIdempotencyTTL.Seconds()),

		enableDebugLogsVal,

		enableThrottleCompatibilityModeVal,
	})
	if err != nil {
		return nil, errs.Wrap(0, false, "invalid args: %w", err)
	}

	l.Trace(
		"prepared acquire call",
	)

	rawRes, err := scripts["acquire"].Exec(ctx, client, keys, args).AsBytes()
	if err != nil {
		return nil, errs.Wrap(0, false, "acquire script failed: %w", err)
	}

	parsedResponse := acquireScriptResponse{}
	err = json.Unmarshal(rawRes, &parsedResponse)
	if err != nil {
		return nil, errs.Wrap(0, false, "invalid response structure: %w", err)
	}

	leases := make([]CapacityLease, len(parsedResponse.GrantedLeases))
	for i, v := range parsedResponse.GrantedLeases {
		// NOTE: To return the original lease idempotency key back to the user,
		// we have to perform a reverse lookup in the map of hash(idempotency key) -> idempotency key
		// we created when serializing the request state.
		leaseIdempotencyKey, ok := hashedLeaseIdempotencyKeysMap[v.LeaseIdempotencyKey]
		if !ok {
			return nil, errs.Wrap(0, false, "invalid hashed lease idempotency key returned")
		}

		leases[i] = CapacityLease{
			LeaseID:        v.LeaseID,
			IdempotencyKey: leaseIdempotencyKey,
		}
	}

	var limitingConstraints []ConstraintItem
	if len(parsedResponse.LimitingConstraints) > 0 {
		limitingConstraints = make([]ConstraintItem, len(parsedResponse.LimitingConstraints))
		for i, limitingConstraintIndex := range []int(parsedResponse.LimitingConstraints) {
			limitingConstraints[i] = sortedConstraints[limitingConstraintIndex-1]
		}
	}

	retryAfter := time.UnixMilli(int64(parsedResponse.RetryAt))
	if retryAfter.Before(now) {
		retryAfter = time.Time{}
	}

	if len(r.lifecycles) > 0 {
		for _, hook := range r.lifecycles {
			err := hook.OnCapacityLeaseAcquired(ctx, OnCapacityLeaseAcquiredData{
				AccountID:           req.AccountID,
				EnvID:               req.EnvID,
				FunctionID:          req.FunctionID,
				Configuration:       req.Configuration,
				Constraints:         req.Constraints,
				LimitingConstraints: limitingConstraints,
				FairnessReduction:   parsedResponse.FairnessReduction,
				RetryAfter:          retryAfter,
				RequestedAmount:     req.Amount,
				Duration:            req.Duration,
				Source:              req.Source,
				GrantedLeases:       leases,
			})
			if err != nil {
				return nil, errs.Wrap(0, false, "acquire lifecycle failed: %w", err)
			}
		}
	}

	l = l.With(
		"status", parsedResponse.Status,
		"active", parsedResponse.ActiveAccountLeases,
		"expired", parsedResponse.ExpiredAccountLeases,
		"earliest_expiry", time.UnixMilli(int64(parsedResponse.EarliestLeaseExpiry)),
	)

	switch parsedResponse.Status {
	case 1, 3:
		l.Trace(
			"successful acquire call",
			"leases", leases,
		)

		// success or idempotency
		return &CapacityAcquireResponse{
			RequestID:           requestID,
			Leases:              leases,
			LimitingConstraints: limitingConstraints,
			FairnessReduction:   parsedResponse.FairnessReduction,
			RetryAfter:          retryAfter,
			internalDebugState:  parsedResponse,
		}, nil

	case 2:
		l.Trace(
			"acquire call lacking capacity",
			"limiting", limitingConstraints,
		)

		// lacking capacity
		return &CapacityAcquireResponse{
			RequestID:           requestID,
			Leases:              leases,
			LimitingConstraints: limitingConstraints,
			RetryAfter:          retryAfter,
			FairnessReduction:   parsedResponse.FairnessReduction,
			internalDebugState:  parsedResponse,
		}, nil

	case 4:
		l.Trace("acquire while previous request state still exists")
		return nil, errs.Wrap(0, false, "request state for this idempotency key already exists")

	default:
		return nil, errs.Wrap(0, false, "unexpected status code %v", parsedResponse.Status)
	}
}
